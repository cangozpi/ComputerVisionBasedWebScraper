{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727114d6-0268-4085-b23a-dae29830048a",
   "metadata": {},
   "source": [
    "Script from https://gist.github.com/saghiralfasly/ee642af0616461145a9a82d7317fb1d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de171e-60ad-44ab-9bd5-8f2d4d2dd159",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5528/1396284595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "'''Python script to create tfrecords from pascal VOC data set format (one class detection) for Object Detection API Tensorflow, where it divides dataset into (90% train.record and 10% test.record)\n",
    "dataset_to_tfrecord.py'''\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "'''\n",
    "this script automatically divides dataset into training and evaluation (10% for evaluation)\n",
    "this scripts also shuffles the dataset before converting it into tfrecords\n",
    "if u have different structure of dataset (rather than pascal VOC ) u need to change\n",
    "the paths and names input directories(images and annotation) and output tfrecords names.\n",
    "(note: this script can be enhanced to use flags instead of changing parameters on code).\n",
    "default expected directories tree:\n",
    "dataset- \n",
    "   -JPEGImages\n",
    "   -Annotations\n",
    "    dataset_to_tfrecord.py   \n",
    "to run this script:\n",
    "$ python dataset_to_tfrecord.py \n",
    "'''\n",
    "def create_example(xml_file):\n",
    "        #process the xml file\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        image_name = root.find('filename').text\n",
    "        file_name = image_name.encode('utf8')\n",
    "        size=root.find('size')\n",
    "        width = int(size[0].text)\n",
    "        height = int(size[1].text)\n",
    "        xmin = []\n",
    "        ymin = []\n",
    "        xmax = []\n",
    "        ymax = []\n",
    "        classes = []\n",
    "        classes_text = []\n",
    "        truncated = []\n",
    "        poses = []\n",
    "        difficult_obj = []\n",
    "        for member in root.findall('object'):\n",
    "           classes_text.append('Person'.encode('utf8'))\n",
    "           xmin.append(float(member[4][0].text) / width)\n",
    "           ymin.append(float(member[4][1].text) / height)\n",
    "           xmax.append(float(member[4][2].text) / width)\n",
    "           ymax.append(float(member[4][3].text) / height)\n",
    "           difficult_obj.append(0)\n",
    "           #if you have more than one classes in dataset you can change the next line\n",
    "           #to read the class from the xml file and change the class label into its \n",
    "           #corresponding integer number, u can use next function structure\n",
    "           '''\n",
    "           def class_text_to_int(row_label):\n",
    "              if row_label == 'Person':\n",
    "                  return 1\n",
    "              if row_label == 'car':\n",
    "                  return 2\n",
    "           and so on.....\n",
    "           '''\n",
    "           classes.append(1)   # i wrote 1 because i have only one class(person)\n",
    "           truncated.append(0)\n",
    "           poses.append('Unspecified'.encode('utf8'))\n",
    "\n",
    "        #read corresponding image\n",
    "        full_path = os.path.join('./JPEGImages', '{}'.format(image_name))  #provide the path of images directory\n",
    "        with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "            encoded_jpg = fid.read()\n",
    "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "        image = Image.open(encoded_jpg_io)\n",
    "        if image.format != 'JPEG':\n",
    "           raise ValueError('Image format not JPEG')\n",
    "        key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "        #create TFRecord Example\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': dataset_util.int64_feature(height),\n",
    "            'image/width': dataset_util.int64_feature(width),\n",
    "            'image/filename': dataset_util.bytes_feature(file_name),\n",
    "            'image/source_id': dataset_util.bytes_feature(file_name),\n",
    "            'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "            'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "            'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "            'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "            'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "            'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "            'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "            'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "            'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "            'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "            'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "        }))\t\n",
    "        return example\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb540c8e-6cdf-4f11-afe1-782ecf7ed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    " writer_train = tf.python_io.TFRecordWriter('train.record')     \n",
    "    writer_test = tf.python_io.TFRecordWriter('test.record')\n",
    "    #provide the path to annotation xml files directory\n",
    "    filename_list=tf.train.match_filenames_once(\"./Annotations/*.xml\")\n",
    "    init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess=tf.Session()\n",
    "    sess.run(init)\n",
    "    list=sess.run(filename_list)\n",
    "    random.shuffle(list)   #shuffle files list\n",
    "    i=1 \n",
    "    tst=0   #to count number of images for evaluation \n",
    "    trn=0   #to count number of images for training\n",
    "    for xml_file in list:\n",
    "      example = create_example(xml_file)\n",
    "      if (i%10)==0:  #each 10th file (xml and image) write it for evaluation\n",
    "         writer_test.write(example.SerializeToString())\n",
    "         tst=tst+1\n",
    "      else:          #the rest for training\n",
    "         writer_train.write(example.SerializeToString())\n",
    "         trn=trn+1\n",
    "      i=i+1\n",
    "      print(xml_file)\n",
    "    writer_test.close()\n",
    "    writer_train.close()\n",
    "    print('Successfully converted dataset to TFRecord.')\n",
    "    print('training dataset: # ')\n",
    "    print(trn)\n",
    "    print('test dataset: # ')\n",
    "    print(tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
